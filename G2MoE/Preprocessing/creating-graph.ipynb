{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13570931,"sourceType":"datasetVersion","datasetId":8621137,"isSourceIdPinned":true},{"sourceId":272464463,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rouge scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T14:57:19.059857Z","iopub.execute_input":"2025-10-31T14:57:19.060063Z","iopub.status.idle":"2025-10-31T14:57:23.396145Z","shell.execute_reply.started":"2025-10-31T14:57:19.060045Z","shell.execute_reply":"2025-10-31T14:57:23.395408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install huggingface_hub==0.33.1 transformers==4.52.4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T14:57:23.397178Z","iopub.execute_input":"2025-10-31T14:57:23.397464Z","iopub.status.idle":"2025-10-31T14:57:35.625276Z","shell.execute_reply.started":"2025-10-31T14:57:23.397435Z","shell.execute_reply":"2025-10-31T14:57:35.624572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport numpy as np\nimport pickle\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nfrom rouge import Rouge\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom tqdm import tqdm\nimport re\n\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\nmodel = AutoModel.from_pretrained(\"vinai/phobert-base\")\nmodel.eval()\n\ndef get_phobert_embedding(text):\n    \"\"\"Get CLS embedding from PhoBERT - returns torch.Tensor\"\"\"\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=256, padding=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state[0][0].unsqueeze(0)\n\nvietnamese_stopwords = set([\n    'bị', 'bởi', 'cả', 'các', 'cái', 'cần', 'càng', 'chỉ', 'chiếc', 'cho', 'chứ',\n    'chưa', 'chuyện', 'có', 'có_thể', 'cứ', 'của', 'cùng', 'cũng', 'đã', 'đang',\n    'đây', 'để', 'đến_nỗi', 'đều', 'điều', 'do', 'đó', 'được', 'dưới', 'gì',\n    'khi', 'không', 'là', 'lại', 'lên', 'lúc', 'mà', 'mỗi', 'này', 'nên', 'nếu',\n    'ngay', 'nhiều', 'như', 'nhưng', 'những', 'nơi', 'nữa', 'phải', 'qua', 'ra',\n    'rằng', 'rất', 'rồi', 'sau', 'sẽ', 'so', 'sự', 'tại', 'theo', 'thì', 'trên',\n    'trước', 'từ', 'từng', 'và', 'vẫn', 'vào', 'vậy', 'vì', 'việc', 'với', 'vừa'\n])\n\nvietnamese_stopwords.update([c for c in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^`{|}~…\"\"\\'\\''])\n\ndef remove_stopwords(text):\n    text = text.lower()\n    words = [w for w in text.split() if w not in vietnamese_stopwords and len(w) > 1]\n    return ' '.join(words)\n\ndef train_lda_model(clusters, n_components=4, max_iter=3):\n    paragraphs = []\n    \n    for cluster in clusters:\n        for doc in cluster['single_documents']:\n            text = doc.get('raw_text', '')\n            if not text:\n                continue\n                \n            paras = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n            \n            if not paras:\n                paras = [p.strip() for p in text.split('\\n') if p.strip()]\n            \n            if not paras:\n                paras = [s.strip() + '.' for s in text.split('.') if s.strip()]\n            \n            paragraphs.extend([remove_stopwords(p) for p in paras if p])\n    \n    tf = TfidfVectorizer(min_df=2, max_df=0.95, max_features=3000, sublinear_tf=True)\n    X = tf.fit_transform(paragraphs)\n    \n    lda_model = LatentDirichletAllocation(\n        n_components=n_components, \n        learning_method='online', \n        random_state=42, \n        max_iter=max_iter\n    )\n    lda_model.fit(X)\n    \n    return tf, lda_model\n\ndef divide_into_sections_lda(doc_texts, tf_model, lda_model):\n    all_sents = []\n    doc_sent_ranges = []\n    doc_paragraphs = []\n    \n    for doc_text in doc_texts:\n        start_idx = len(all_sents)\n        sents = [s.strip() for s in doc_text.split('\\n') if s.strip()]\n        if not sents:\n            sents = [s.strip() + '.' for s in doc_text.split('.') if s.strip()]\n        all_sents.extend(sents)\n        paras = []\n        para_sent_indices = []\n        for i in range(0, len(sents), 4):\n            para = ' '.join(sents[i:i+4])\n            paras.append(para)\n            para_sent_indices.append(list(range(start_idx + i, start_idx + min(i+4, len(sents)))))\n        doc_paragraphs.append((paras, para_sent_indices))\n        doc_sent_ranges.append((start_idx, len(all_sents)))\n    \n    all_paras = []\n    para_doc_map = []\n    para_sent_map = []\n    \n    for doc_idx, (paras, sent_indices) in enumerate(doc_paragraphs):\n        for para, sent_idx in zip(paras, sent_indices):\n            all_paras.append(remove_stopwords(para))\n            para_doc_map.append(doc_idx)\n            para_sent_map.append(sent_idx)\n    \n    X = tf_model.transform(all_paras)\n    lda_topics = lda_model.transform(X)\n    \n    para_topics = []\n    for topic_dist in lda_topics:\n        if np.min(topic_dist) == np.max(topic_dist):\n            para_topics.append(0)\n        else:\n            para_topics.append(np.argmax(topic_dist))\n    \n    section_map = {}\n    sect_id = 0\n    \n    for doc_idx in range(len(doc_texts)):\n        doc_topics = set()\n        for para_idx, (p_doc, p_topic) in enumerate(zip(para_doc_map, para_topics)):\n            if p_doc == doc_idx:\n                doc_topics.add(p_topic)\n        \n        for topic in sorted(doc_topics):\n            section_map[(doc_idx, topic)] = sect_id\n            sect_id += 1\n    \n    total_sects = sect_id\n    total_sents = len(all_sents)\n    total_docs = len(doc_texts)\n    \n    doc_sect_mask = np.zeros((total_docs, total_sects), dtype=int)\n    sect_sent_mask = np.zeros((total_sects, total_sents), dtype=int)\n    \n    for para_idx, (p_doc, p_topic, sent_indices) in enumerate(zip(para_doc_map, para_topics, para_sent_map)):\n        global_sect_id = section_map[(p_doc, p_topic)]\n        doc_sect_mask[p_doc][global_sect_id] = 1\n        for sent_idx in sent_indices:\n            sect_sent_mask[global_sect_id][sent_idx] = 1\n    \n    return doc_sect_mask, sect_sent_mask, all_sents\n\ndef mask_to_adj(doc_sect_mask, sect_sent_mask):\n    doc_sect_mask = np.array(doc_sect_mask)\n    sect_sent_mask = np.array(sect_sent_mask)\n    \n    sent_num = sect_sent_mask.shape[1]\n    sect_num = sect_sent_mask.shape[0]\n    doc_num = doc_sect_mask.shape[0]\n    total_nodes = sent_num + sect_num + doc_num + 1\n    \n    adj = np.zeros((total_nodes, total_nodes))\n    \n    adj[sent_num:sent_num+sect_num, 0:sent_num] = sect_sent_mask\n    adj[0:sent_num, sent_num:sent_num+sect_num] = sect_sent_mask.T\n    \n    adj[sent_num+sect_num:sent_num+sect_num+doc_num, sent_num:sent_num+sect_num] = doc_sect_mask\n    adj[sent_num:sent_num+sect_num, sent_num+sect_num:sent_num+sect_num+doc_num] = doc_sect_mask.T\n    \n    for i in range(sect_num):\n        sect_mask = sect_sent_mask[i:i+1]\n        adj[0:sent_num, 0:sent_num] += sect_mask.T @ sect_mask\n    \n    for i in range(doc_num):\n        doc_mask = doc_sect_mask[i:i+1]\n        adj[sent_num:sent_num+sect_num, sent_num:sent_num+sect_num] += doc_mask.T @ doc_mask\n    \n    root_idx = total_nodes - 1\n    adj[root_idx, sent_num+sect_num:sent_num+sect_num+doc_num] = 1\n    adj[sent_num+sect_num:sent_num+sect_num+doc_num, root_idx] = 1\n    adj[root_idx, root_idx] = 1\n    \n    return adj\n\nclass Graph:\n    def __init__(self, sents, sentVecs, scores, doc_sec_mask, sec_sen_mask, golden, threds=0.5):\n        assert len(sentVecs) == len(scores) == len(sents), \\\n            f\"Mismatch: {len(sentVecs)} vecs, {len(scores)} scores, {len(sents)} sents\"\n        \n        self.docnum = len(doc_sec_mask)\n        self.secnum = len(sec_sen_mask)\n        self.adj = torch.from_numpy(mask_to_adj(doc_sec_mask, sec_sen_mask)).float()\n        \n        sentVecs_np = []\n        for vec in sentVecs:\n            if isinstance(vec, torch.Tensor):\n                sentVecs_np.append(vec.squeeze().cpu().numpy())\n            else:\n                sentVecs_np.append(vec)\n        \n        vec_dim = sentVecs_np[0].shape[0]\n        self.feature = np.concatenate((\n            np.array(sentVecs_np), \n            np.zeros((self.secnum + self.docnum + 1, vec_dim))\n        ))\n        \n        self.score = torch.from_numpy(np.array(scores)).float()\n        self.score_onehot = (self.score >= threds).float()\n        self.sents = np.array(sents)\n        self.golden = golden\n        \n        golden_embedding = get_phobert_embedding(golden)\n        self.goldenVec = golden_embedding.float()\n        \n        self.init_node_vec()\n        self.feature = torch.from_numpy(self.feature).float()\n    \n    def init_node_vec(self):\n        sent_num = len(self.sents)\n        \n        for i in range(sent_num, sent_num + self.secnum):\n            mask = self.adj[i].clone()\n            mask[sent_num:] = 0\n            connected = mask.bool()\n            if connected.any():\n                connected_indices = connected.numpy()\n                self.feature[i] = np.mean(self.feature[connected_indices], axis=0)\n        \n        for i in range(sent_num + self.secnum, sent_num + self.secnum + self.docnum):\n            mask = self.adj[i].clone()\n            mask[sent_num + self.secnum:] = 0\n            connected = mask.bool()\n            if connected.any():\n                connected_indices = connected.numpy()\n                self.feature[i] = np.mean(self.feature[connected_indices], axis=0)\n        \n        doc_start = sent_num + self.secnum\n        doc_end = sent_num + self.secnum + self.docnum\n        self.feature[-1] = np.mean(self.feature[doc_start:doc_end], axis=0)\n\ndef process_jsonl_to_graphs(jsonl_path, use_pretrained_lda=None, n_components=4, max_iter=3, \n                            start_line=0, end_line=None):\n    clusters = []\n    with open(jsonl_path, 'r', encoding='utf-8') as f:\n        for idx, line in enumerate(f):\n            if idx < start_line:\n                continue\n            if end_line is not None and idx >= end_line:\n                break\n            clusters.append(json.loads(line))\n    \n    if use_pretrained_lda is None:\n        tf_model, lda_model = train_lda_model(clusters, n_components, max_iter)\n    else:\n        tf_model, lda_model = use_pretrained_lda\n    \n    rouge = Rouge()\n    \n    summary_types = ['summary_0', 'summary_1', 's3_summary_0', 's3_summary_1']\n    all_graphs = {stype: [] for stype in summary_types}\n    \n    for cluster_idx, cluster in enumerate(tqdm(clusters, desc=\"Processing clusters\")):\n        try:\n            doc_texts = [doc.get('raw_text', '') for doc in cluster['single_documents']]\n            doc_texts = [text for text in doc_texts if text]\n            \n            if not doc_texts:\n                continue\n            \n            doc_sect_mask, sect_sent_mask, sents = divide_into_sections_lda(\n                doc_texts, tf_model, lda_model\n            )\n            \n            if not sents:\n                continue\n            \n            sentVecs = [get_phobert_embedding(sent) for sent in sents]\n            \n            for stype in summary_types:\n                summary = cluster.get(stype, '')\n                \n                if not summary:\n                    continue\n                \n                summary = summary.replace('– ', '').replace('- ', '').strip()\n                \n                if not summary:\n                    continue\n                \n                scores = []\n                for sent in sents:\n                    if not sent.strip():\n                        scores.append(0.0)\n                        continue\n                    \n                    try:\n                        rouge_scores = rouge.get_scores(sent, summary)[0]\n                        score = rouge_scores['rouge-2']['p']\n                        scores.append(score)\n                    except:\n                        scores.append(0.0)\n                \n                graph = Graph(sents, sentVecs, scores, doc_sect_mask, sect_sent_mask, summary)\n                all_graphs[stype].append(graph)\n        \n        except Exception:\n            continue\n    \n    return all_graphs, (tf_model, lda_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T14:57:35.629316Z","iopub.execute_input":"2025-10-31T14:57:35.629547Z","iopub.status.idle":"2025-10-31T14:58:09.398161Z","shell.execute_reply.started":"2025-10-31T14:57:35.629527Z","shell.execute_reply":"2025-10-31T14:58:09.397521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\nimport sys\nimport pickle\n\ninput_path = ''\nlda_model_path = None\nsave_prefix = ''\nsamples_per_group = 500\ngroup_to_run = 1\n\nlda_models_loaded = None\nif lda_model_path:\n    try:\n        with open(lda_model_path, 'rb') as f:\n            lda_models_loaded = pickle.load(f)\n    except Exception:\n        pass\n\nwith open(input_path, 'r', encoding='utf-8') as f:\n    total_lines = sum(1 for _ in f)\n\ntotal_groups = math.ceil(total_lines / samples_per_group)\n\nif not (1 <= group_to_run <= total_groups):\n    sys.exit(1)\n\ngroup_idx0 = group_to_run - 1\nstart_line = group_idx0 * samples_per_group\nend_line = min((group_idx0 + 1) * samples_per_group, total_lines)\n\nall_graphs, lda_models = process_jsonl_to_graphs(\n    input_path,\n    use_pretrained_lda=lda_models_loaded,\n    n_components=4,\n    max_iter=3,\n    start_line=start_line,\n    end_line=end_line\n)\n\nfor stype, graphs in all_graphs.items():\n    if graphs:\n        output_name = f\"{save_prefix}_{stype}_{group_to_run}_numsample{samples_per_group}_numgroup{total_groups}.pkl\"\n        with open(output_name, 'wb') as f:\n            pickle.dump(graphs, f)\n\nif group_to_run == 1 and lda_models_loaded is None:\n    lda_save_path = f\"{save_prefix}_lda_models.pkl\"\n    with open(lda_save_path, 'wb') as f:\n        pickle.dump(lda_models, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T15:01:58.837110Z","iopub.execute_input":"2025-10-31T15:01:58.837721Z","execution_failed":"2025-10-31T15:03:43.332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}